{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9148d141-5546-412e-ba82-ab1e7365dea9",
   "metadata": {},
   "source": [
    "Tokenization with Hugging Face Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b341829c-02f1-40df-a5af-bc5537486cdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81187b96-1fbb-4b16-8343-e3f542a7daa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "943dd1c3-2689-47d3-8742-7fa63274b923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efe9eac5bb454de681d59cd9357748b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8ee85ac5bc14ee7a8490a580666ae28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "534d450c74c9461fa9e28a8130dc62b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "852a03896823499fbc2ff171b1c4266b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33096467-595f-4cb6-b57a-0ac4522652be",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"BERT preprocessing is essential.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27bc0ad2-2883-45ff-8ef7-e5be4ade8645",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fea06b8-4568-4645-80ef-3490a5f5b85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bert', 'prep', '##ro', '##ces', '##sing', 'is', 'essential', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4038ab79-aa65-4715-afad-ff3238912ac2",
   "metadata": {},
   "source": [
    "Text Classification with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a99a3fbe-29e5-4f5b-ac87-372448985041",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f864bcc-d0c1-4e9e-9ac4-6d828e234c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9c46b6c-606c-4623-a172-68e8de2e845d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dba5956e-0b35-4268-8bba-dab175bfea45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d45b16ce365455fafaa328ac275f863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02b6ccad-1f54-4407-997d-9371c748947c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Artificial Intelligence is the simulation of human intelligence process by Machines\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9351341e-7990-4086-b83a-95d89f45f7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0cb786dd-4ebf-454b-a07b-fdabd6eb9273",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = torch.argmax(outputs.logits, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "42b9423f-d561-401b-9b35-ff375303712c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1])\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b49d7c6-84ad-49e8-949a-8453f6acb2c0",
   "metadata": {},
   "source": [
    "Visualizing Attention Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "93b45e84-2d58-4a35-820c-6755359dab68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e6946f26-79bd-4778-92e1-36cbbe6d9f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c333000b-4173-49f4-8685-6859c4211d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Machine Learning is a branch of Artificial Intelligence\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4c9a3c23-f384-4cab-97ac-d8dc4f3a5a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "outputs = model(**inputs, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9aec31ab-79fb-47e9-a854-bb3deb6c9251",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_weights = outputs.attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "174e5384-d1ed-433e-80b2-4566c5d6bb29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[[6.3404e-02, 7.4812e-02, 5.0517e-02,  ..., 5.8008e-02,\n",
      "           3.8826e-02, 3.2250e-01],\n",
      "          [3.3223e-02, 8.5746e-02, 1.7019e-01,  ..., 1.3015e-01,\n",
      "           1.6430e-01, 1.6220e-01],\n",
      "          [7.1231e-02, 1.3327e-01, 6.6658e-02,  ..., 1.9794e-01,\n",
      "           1.3782e-01, 8.5812e-02],\n",
      "          ...,\n",
      "          [7.2923e-02, 1.8865e-01, 2.3325e-01,  ..., 1.0993e-01,\n",
      "           1.5193e-01, 5.1124e-02],\n",
      "          [4.9920e-02, 1.7651e-01, 1.0689e-01,  ..., 2.6957e-01,\n",
      "           6.5506e-02, 6.6842e-02],\n",
      "          [1.0438e-01, 7.2058e-02, 7.4685e-02,  ..., 8.7407e-02,\n",
      "           4.5061e-02, 2.0507e-01]],\n",
      "\n",
      "         [[5.2830e-01, 3.6457e-03, 1.7123e-03,  ..., 3.4617e-03,\n",
      "           4.2933e-03, 5.6448e-03],\n",
      "          [5.4541e-03, 6.7088e-02, 1.7184e-01,  ..., 6.8875e-02,\n",
      "           4.2888e-01, 1.3295e-01],\n",
      "          [6.5564e-03, 2.1092e-01, 8.8582e-02,  ..., 4.2514e-01,\n",
      "           1.3918e-01, 3.3715e-02],\n",
      "          ...,\n",
      "          [2.7423e-02, 5.5378e-02, 1.0394e-01,  ..., 2.7195e-02,\n",
      "           2.9683e-01, 3.5296e-01],\n",
      "          [4.0793e-03, 1.1702e-01, 1.1868e-01,  ..., 4.5975e-01,\n",
      "           7.5008e-02, 5.0950e-02],\n",
      "          [6.0064e-02, 4.0091e-02, 3.5619e-02,  ..., 2.3799e-02,\n",
      "           9.4584e-02, 5.8511e-02]],\n",
      "\n",
      "         [[8.0123e-01, 1.5460e-02, 1.8684e-02,  ..., 5.8505e-03,\n",
      "           2.9066e-02, 5.2343e-02],\n",
      "          [2.1454e-01, 2.4658e-02, 3.0193e-01,  ..., 9.9725e-05,\n",
      "           7.9936e-02, 1.7545e-02],\n",
      "          [5.9672e-02, 9.1801e-01, 8.3801e-04,  ..., 1.2218e-02,\n",
      "           1.3549e-05, 1.3879e-03],\n",
      "          ...,\n",
      "          [2.4648e-01, 1.6273e-03, 1.2122e-02,  ..., 6.7317e-04,\n",
      "           5.8136e-01, 1.6996e-02],\n",
      "          [7.9349e-03, 1.1004e-03, 1.0257e-04,  ..., 9.8725e-01,\n",
      "           4.9401e-05, 1.4109e-03],\n",
      "          [3.7791e-01, 5.3187e-04, 4.1126e-03,  ..., 1.4817e-02,\n",
      "           3.9263e-01, 1.6035e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.1934e-01, 2.7731e-01, 3.0897e-02,  ..., 7.9738e-02,\n",
      "           3.2345e-02, 2.1870e-02],\n",
      "          [1.0783e-01, 3.6758e-01, 7.2104e-02,  ..., 9.8884e-02,\n",
      "           9.0214e-02, 1.1573e-01],\n",
      "          [6.5698e-02, 8.8288e-02, 2.3057e-01,  ..., 1.6382e-01,\n",
      "           2.9291e-01, 4.8342e-02],\n",
      "          ...,\n",
      "          [1.5825e-01, 1.5422e-01, 5.3545e-02,  ..., 1.7771e-01,\n",
      "           3.2826e-02, 3.2602e-02],\n",
      "          [2.0196e-01, 1.1706e-01, 7.7718e-02,  ..., 2.8437e-01,\n",
      "           2.2056e-01, 3.0254e-02],\n",
      "          [5.6941e-01, 1.0703e-01, 3.7960e-02,  ..., 1.9434e-02,\n",
      "           7.9222e-03, 3.0270e-02]],\n",
      "\n",
      "         [[8.1722e-01, 1.2884e-02, 1.9138e-02,  ..., 2.1769e-02,\n",
      "           2.9109e-02, 2.5691e-02],\n",
      "          [2.6704e-02, 3.5681e-03, 9.2158e-01,  ..., 9.7846e-04,\n",
      "           3.3695e-03, 1.7888e-04],\n",
      "          [1.5309e-01, 1.9448e-01, 9.2739e-03,  ..., 6.9714e-03,\n",
      "           1.6638e-03, 1.5834e-02],\n",
      "          ...,\n",
      "          [4.8738e-03, 1.6249e-05, 1.1193e-04,  ..., 4.2088e-04,\n",
      "           9.8180e-01, 1.0136e-02],\n",
      "          [5.3467e-02, 9.0224e-04, 2.3224e-04,  ..., 9.7811e-02,\n",
      "           6.2640e-03, 8.0678e-01],\n",
      "          [2.5090e-01, 2.8710e-03, 3.5531e-03,  ..., 8.8611e-02,\n",
      "           2.1913e-01, 4.0634e-01]],\n",
      "\n",
      "         [[9.0388e-01, 9.6102e-03, 2.4475e-03,  ..., 1.2145e-03,\n",
      "           1.4341e-03, 3.0923e-02],\n",
      "          [2.6938e-01, 2.6573e-02, 2.4095e-01,  ..., 1.2910e-02,\n",
      "           8.5819e-02, 1.0458e-01],\n",
      "          [2.6701e-02, 9.0277e-01, 2.6122e-03,  ..., 1.7762e-02,\n",
      "           4.7177e-03, 5.9967e-03],\n",
      "          ...,\n",
      "          [2.2652e-01, 5.8682e-02, 6.1606e-02,  ..., 2.2306e-02,\n",
      "           2.2453e-01, 1.4760e-01],\n",
      "          [2.3383e-03, 1.5650e-02, 2.1203e-03,  ..., 9.7000e-01,\n",
      "           4.9748e-04, 5.9890e-03],\n",
      "          [7.2694e-01, 1.1485e-02, 5.7634e-03,  ..., 1.3093e-02,\n",
      "           1.6613e-02, 1.6920e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[3.4758e-01, 8.0150e-02, 6.9526e-02,  ..., 5.3591e-02,\n",
      "           3.7260e-02, 5.5351e-02],\n",
      "          [1.5798e-01, 3.8268e-02, 3.2143e-02,  ..., 4.4156e-02,\n",
      "           1.2612e-01, 4.3427e-01],\n",
      "          [1.3134e-01, 1.8043e-01, 1.7195e-02,  ..., 9.5831e-02,\n",
      "           4.3501e-02, 3.3150e-01],\n",
      "          ...,\n",
      "          [1.0386e-01, 1.3925e-01, 1.5543e-02,  ..., 8.7666e-02,\n",
      "           1.4213e-01, 3.7042e-01],\n",
      "          [3.2340e-02, 2.5577e-01, 2.2881e-02,  ..., 1.9775e-01,\n",
      "           1.6350e-01, 2.0968e-01],\n",
      "          [5.5482e-01, 6.9567e-02, 5.4422e-02,  ..., 3.3761e-02,\n",
      "           3.0684e-02, 4.2663e-02]],\n",
      "\n",
      "         [[5.9461e-01, 2.7850e-02, 5.5777e-02,  ..., 1.3780e-02,\n",
      "           5.2715e-02, 8.6724e-02],\n",
      "          [5.8505e-02, 9.9898e-04, 8.7583e-01,  ..., 3.5346e-04,\n",
      "           2.5381e-03, 1.3273e-03],\n",
      "          [3.9605e-01, 7.0698e-02, 1.2238e-02,  ..., 3.1818e-03,\n",
      "           3.2510e-02, 2.1443e-02],\n",
      "          ...,\n",
      "          [7.1855e-02, 4.6588e-05, 3.5165e-03,  ..., 5.0929e-03,\n",
      "           8.1725e-01, 6.1054e-02],\n",
      "          [3.1912e-01, 9.7358e-04, 1.8468e-03,  ..., 6.9377e-03,\n",
      "           1.3567e-02, 6.3773e-01],\n",
      "          [9.0755e-01, 9.2333e-04, 5.3262e-04,  ..., 2.3028e-03,\n",
      "           1.0459e-03, 8.5256e-02]],\n",
      "\n",
      "         [[8.4719e-01, 1.6170e-02, 1.3817e-02,  ..., 1.3011e-02,\n",
      "           2.0739e-02, 3.1719e-02],\n",
      "          [8.0549e-01, 1.9621e-03, 2.7815e-02,  ..., 3.5468e-02,\n",
      "           7.7458e-03, 3.2260e-02],\n",
      "          [8.7797e-01, 7.1788e-03, 3.5058e-03,  ..., 1.6688e-02,\n",
      "           3.4820e-03, 1.1639e-02],\n",
      "          ...,\n",
      "          [8.8834e-01, 4.4524e-03, 1.0423e-02,  ..., 9.7105e-03,\n",
      "           3.4735e-02, 1.0587e-02],\n",
      "          [6.7857e-01, 8.3178e-03, 1.2572e-02,  ..., 6.1917e-02,\n",
      "           3.0031e-02, 1.5584e-02],\n",
      "          [7.4577e-01, 1.8149e-02, 2.9237e-02,  ..., 1.7703e-02,\n",
      "           2.3488e-02, 8.0052e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[3.3874e-01, 4.4714e-02, 5.3284e-02,  ..., 5.8589e-02,\n",
      "           7.5324e-02, 1.3254e-01],\n",
      "          [5.7626e-01, 5.4147e-03, 3.0514e-02,  ..., 2.9322e-02,\n",
      "           1.9459e-02, 1.9490e-01],\n",
      "          [7.9660e-01, 2.6659e-02, 1.3796e-03,  ..., 3.5683e-02,\n",
      "           1.0411e-02, 8.4376e-02],\n",
      "          ...,\n",
      "          [6.6577e-01, 1.5851e-02, 2.8237e-02,  ..., 1.3357e-03,\n",
      "           1.8500e-02, 1.0603e-01],\n",
      "          [5.6210e-01, 4.3503e-02, 2.0703e-02,  ..., 3.4154e-02,\n",
      "           7.1768e-03, 1.8856e-01],\n",
      "          [1.4618e-01, 9.3343e-02, 7.7780e-02,  ..., 1.4125e-01,\n",
      "           8.9577e-02, 1.3928e-01]],\n",
      "\n",
      "         [[7.4845e-01, 1.4675e-02, 1.9867e-02,  ..., 1.2174e-02,\n",
      "           1.4040e-02, 8.9082e-02],\n",
      "          [6.9138e-01, 9.2328e-03, 1.3780e-01,  ..., 1.0227e-02,\n",
      "           1.6105e-02, 2.8556e-02],\n",
      "          [4.1106e-01, 1.7375e-01, 4.2759e-02,  ..., 3.8704e-02,\n",
      "           4.2280e-02, 5.6699e-02],\n",
      "          ...,\n",
      "          [3.6276e-01, 2.3870e-02, 1.2017e-01,  ..., 1.0740e-02,\n",
      "           5.0727e-02, 1.6641e-01],\n",
      "          [1.7104e-01, 8.5467e-02, 1.0210e-01,  ..., 8.4458e-02,\n",
      "           4.5074e-02, 9.7616e-02],\n",
      "          [8.5374e-01, 5.0465e-03, 6.5098e-03,  ..., 6.1687e-03,\n",
      "           6.5044e-03, 5.3832e-02]],\n",
      "\n",
      "         [[4.0649e-01, 2.1518e-02, 3.6528e-02,  ..., 5.1116e-02,\n",
      "           5.0651e-02, 1.8932e-01],\n",
      "          [1.4526e-01, 4.8432e-01, 9.5951e-03,  ..., 1.0720e-01,\n",
      "           7.7867e-02, 1.5080e-01],\n",
      "          [1.9365e-01, 6.9477e-03, 4.9752e-01,  ..., 4.3671e-02,\n",
      "           6.9121e-02, 1.6478e-01],\n",
      "          ...,\n",
      "          [1.1385e-01, 1.0609e-01, 4.5518e-02,  ..., 4.9709e-01,\n",
      "           1.1883e-01, 5.3433e-02],\n",
      "          [5.7989e-02, 3.8430e-02, 8.4139e-02,  ..., 4.8554e-02,\n",
      "           6.5143e-01, 4.6783e-02],\n",
      "          [4.4936e-01, 6.3564e-02, 5.7869e-02,  ..., 4.7226e-02,\n",
      "           4.0697e-02, 8.2291e-02]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[9.8235e-01, 2.1253e-04, 1.0906e-03,  ..., 1.9020e-04,\n",
      "           1.4115e-03, 1.2879e-02],\n",
      "          [5.0410e-06, 3.2898e-05, 9.9994e-01,  ..., 6.4535e-07,\n",
      "           5.2881e-06, 2.7447e-09],\n",
      "          [1.3563e-03, 1.3064e-05, 1.8721e-04,  ..., 1.7222e-08,\n",
      "           4.5750e-06, 1.6328e-05],\n",
      "          ...,\n",
      "          [7.9836e-06, 7.5410e-10, 9.5783e-07,  ..., 1.4913e-04,\n",
      "           9.9983e-01, 7.5051e-06],\n",
      "          [7.3297e-03, 2.4889e-07, 1.7288e-08,  ..., 2.1271e-05,\n",
      "           2.0476e-04, 9.9237e-01],\n",
      "          [9.8285e-01, 2.8432e-05, 9.8811e-05,  ..., 5.2326e-05,\n",
      "           1.0855e-04, 1.6826e-02]],\n",
      "\n",
      "         [[6.5904e-01, 9.0593e-03, 1.8967e-02,  ..., 2.1381e-02,\n",
      "           1.3559e-02, 1.4842e-01],\n",
      "          [1.2918e-01, 2.3487e-04, 8.3316e-01,  ..., 1.3815e-03,\n",
      "           1.0911e-02, 1.6694e-02],\n",
      "          [5.2135e-02, 8.7252e-01, 3.5157e-04,  ..., 3.5678e-02,\n",
      "           7.9557e-05, 2.4096e-02],\n",
      "          ...,\n",
      "          [2.3421e-01, 3.8896e-03, 1.1669e-03,  ..., 1.7248e-02,\n",
      "           6.3234e-01, 8.3733e-02],\n",
      "          [4.5568e-03, 2.5482e-04, 1.0951e-05,  ..., 9.8151e-01,\n",
      "           7.3582e-06, 9.5849e-03],\n",
      "          [7.9509e-01, 1.2590e-03, 1.3312e-03,  ..., 2.8022e-03,\n",
      "           2.1588e-03, 1.7458e-01]],\n",
      "\n",
      "         [[7.8488e-01, 4.5969e-03, 9.7000e-03,  ..., 9.9833e-03,\n",
      "           4.6077e-03, 1.0221e-01],\n",
      "          [4.7009e-01, 6.0728e-03, 6.5843e-02,  ..., 1.0359e-01,\n",
      "           2.7361e-02, 1.5150e-01],\n",
      "          [5.2494e-01, 1.8003e-02, 2.5006e-02,  ..., 1.3251e-02,\n",
      "           8.1693e-03, 3.0196e-01],\n",
      "          ...,\n",
      "          [8.1967e-01, 3.3949e-02, 1.8481e-02,  ..., 8.8477e-03,\n",
      "           4.6605e-03, 4.9254e-02],\n",
      "          [2.6303e-01, 4.6708e-02, 5.0340e-02,  ..., 3.9935e-02,\n",
      "           7.8831e-03, 1.9187e-01],\n",
      "          [6.0306e-01, 2.7205e-02, 2.8875e-02,  ..., 2.3899e-02,\n",
      "           1.2468e-02, 1.7916e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[7.3515e-01, 5.8756e-03, 1.2351e-02,  ..., 3.4835e-03,\n",
      "           6.5098e-03, 2.1011e-01],\n",
      "          [3.1342e-06, 3.3810e-05, 9.9993e-01,  ..., 4.7279e-07,\n",
      "           1.3383e-06, 2.8557e-09],\n",
      "          [1.4450e-03, 4.8068e-06, 5.5714e-04,  ..., 8.1664e-09,\n",
      "           7.2477e-06, 6.1686e-05],\n",
      "          ...,\n",
      "          [5.2393e-06, 9.7781e-10, 2.9533e-07,  ..., 1.2240e-04,\n",
      "           9.9986e-01, 1.0305e-05],\n",
      "          [2.0022e-03, 7.1225e-08, 1.6435e-08,  ..., 1.6672e-06,\n",
      "           1.1193e-04, 9.9786e-01],\n",
      "          [9.9330e-01, 9.9088e-06, 3.8569e-05,  ..., 3.1572e-05,\n",
      "           1.5757e-04, 6.4533e-03]],\n",
      "\n",
      "         [[6.6140e-01, 1.9059e-02, 1.8201e-02,  ..., 2.1735e-02,\n",
      "           1.1177e-02, 1.4219e-01],\n",
      "          [1.0392e-01, 2.6145e-02, 5.6303e-01,  ..., 1.1500e-01,\n",
      "           1.5373e-01, 1.0343e-02],\n",
      "          [2.9074e-02, 5.6721e-02, 6.5519e-01,  ..., 6.6216e-02,\n",
      "           6.4837e-02, 1.9083e-02],\n",
      "          ...,\n",
      "          [3.8804e-02, 5.4545e-02, 8.6468e-02,  ..., 9.0546e-02,\n",
      "           6.9833e-01, 1.8312e-03],\n",
      "          [3.8122e-02, 7.8698e-02, 2.4570e-01,  ..., 4.7433e-01,\n",
      "           4.0297e-02, 1.5033e-02],\n",
      "          [8.7051e-01, 3.3626e-03, 6.6999e-03,  ..., 6.8726e-03,\n",
      "           3.6186e-03, 5.5612e-02]],\n",
      "\n",
      "         [[9.5173e-01, 6.5209e-04, 3.0429e-03,  ..., 1.1131e-03,\n",
      "           1.4468e-03, 2.5510e-02],\n",
      "          [8.5231e-01, 4.3752e-03, 3.0817e-02,  ..., 9.8899e-03,\n",
      "           3.6699e-03, 3.8364e-02],\n",
      "          [4.7175e-01, 6.9663e-02, 2.6657e-02,  ..., 5.2341e-02,\n",
      "           2.2139e-02, 3.4109e-02],\n",
      "          ...,\n",
      "          [9.4478e-01, 1.6380e-03, 6.4372e-03,  ..., 5.3264e-03,\n",
      "           6.9338e-03, 2.5115e-02],\n",
      "          [8.6864e-01, 5.6727e-03, 1.9262e-02,  ..., 2.5445e-02,\n",
      "           1.2875e-02, 3.7653e-02],\n",
      "          [9.8882e-01, 2.5646e-04, 8.8159e-04,  ..., 3.0575e-04,\n",
      "           5.0830e-04, 5.1426e-03]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[3.6918e-01, 8.9386e-04, 1.6138e-03,  ..., 1.0000e-03,\n",
      "           1.4686e-03, 6.0156e-01],\n",
      "          [2.0450e-01, 5.7793e-01, 5.3426e-05,  ..., 3.1486e-03,\n",
      "           3.4109e-03, 2.0895e-01],\n",
      "          [3.9838e-01, 8.8868e-05, 2.0001e-01,  ..., 2.1466e-04,\n",
      "           8.5607e-03, 3.8927e-01],\n",
      "          ...,\n",
      "          [5.0978e-01, 2.0768e-03, 3.0465e-04,  ..., 8.9781e-02,\n",
      "           2.1282e-03, 3.9101e-01],\n",
      "          [2.8952e-01, 2.6971e-03, 2.0558e-02,  ..., 2.4001e-03,\n",
      "           4.0421e-01, 2.7622e-01],\n",
      "          [3.2382e-01, 1.2919e-03, 3.4577e-03,  ..., 1.9981e-03,\n",
      "           3.1730e-03, 6.4365e-01]],\n",
      "\n",
      "         [[3.2412e-01, 1.0416e-02, 1.6469e-02,  ..., 1.3851e-02,\n",
      "           2.6451e-02, 5.2189e-01],\n",
      "          [3.7826e-01, 2.6131e-03, 9.0071e-03,  ..., 4.2562e-03,\n",
      "           5.8686e-03, 4.3083e-01],\n",
      "          [4.1933e-01, 1.0734e-02, 1.4848e-02,  ..., 9.5736e-03,\n",
      "           2.8127e-02, 2.2173e-01],\n",
      "          ...,\n",
      "          [4.6744e-01, 1.7824e-02, 1.9557e-02,  ..., 4.1157e-03,\n",
      "           1.3613e-02, 2.5395e-01],\n",
      "          [1.6978e-01, 1.2780e-02, 1.3506e-02,  ..., 5.6138e-03,\n",
      "           1.9149e-02, 6.3958e-01],\n",
      "          [5.2163e-01, 5.9675e-04, 2.6865e-03,  ..., 9.8000e-04,\n",
      "           2.9249e-03, 4.6336e-01]],\n",
      "\n",
      "         [[7.4398e-02, 5.8758e-02, 1.1674e-01,  ..., 1.2980e-01,\n",
      "           7.7138e-02, 1.3489e-01],\n",
      "          [4.7735e-01, 3.4254e-03, 2.3480e-02,  ..., 3.7421e-02,\n",
      "           5.5367e-02, 3.3919e-01],\n",
      "          [4.4571e-01, 1.4345e-02, 2.7131e-02,  ..., 1.9881e-02,\n",
      "           2.7861e-02, 3.8290e-01],\n",
      "          ...,\n",
      "          [5.1804e-01, 1.4334e-02, 1.5466e-02,  ..., 2.5026e-02,\n",
      "           8.4578e-02, 2.3123e-01],\n",
      "          [5.1925e-01, 8.0097e-03, 1.9246e-02,  ..., 2.6011e-02,\n",
      "           2.6490e-02, 2.9370e-01],\n",
      "          [1.4395e-01, 6.8885e-03, 8.9896e-03,  ..., 1.3407e-02,\n",
      "           4.0940e-03, 7.5537e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[7.2234e-01, 3.6559e-03, 4.1556e-02,  ..., 9.2162e-03,\n",
      "           1.7531e-02, 1.3771e-01],\n",
      "          [3.2993e-01, 3.9423e-04, 3.8100e-01,  ..., 3.8983e-05,\n",
      "           6.8688e-03, 2.7510e-01],\n",
      "          [9.1047e-01, 1.2323e-04, 7.9577e-03,  ..., 8.6458e-05,\n",
      "           1.1762e-03, 5.6107e-02],\n",
      "          ...,\n",
      "          [1.6637e-01, 8.0265e-05, 5.3539e-03,  ..., 2.3535e-03,\n",
      "           6.3066e-01, 1.9322e-01],\n",
      "          [7.6084e-01, 2.4145e-05, 1.2621e-03,  ..., 2.1762e-03,\n",
      "           5.6425e-03, 2.1646e-01],\n",
      "          [9.1279e-01, 2.4946e-03, 4.7352e-03,  ..., 1.9902e-03,\n",
      "           1.7328e-03, 5.5509e-02]],\n",
      "\n",
      "         [[3.1500e-01, 3.2685e-03, 1.9750e-02,  ..., 6.9218e-03,\n",
      "           1.0257e-02, 6.1413e-01],\n",
      "          [5.1244e-01, 9.9947e-04, 1.5962e-01,  ..., 1.3419e-03,\n",
      "           7.6392e-03, 3.1422e-01],\n",
      "          [4.9239e-01, 1.8242e-01, 7.3869e-03,  ..., 4.5376e-03,\n",
      "           3.9333e-03, 2.8654e-01],\n",
      "          ...,\n",
      "          [4.6269e-01, 5.1508e-03, 6.3524e-02,  ..., 4.2758e-03,\n",
      "           2.6571e-01, 1.9419e-01],\n",
      "          [3.2081e-01, 1.7138e-01, 1.4949e-02,  ..., 1.7033e-01,\n",
      "           7.0327e-03, 2.7818e-01],\n",
      "          [3.0396e-01, 5.3751e-04, 1.2470e-03,  ..., 9.0096e-04,\n",
      "           1.4490e-03, 6.8690e-01]],\n",
      "\n",
      "         [[6.0213e-01, 6.8063e-03, 7.6755e-03,  ..., 1.0406e-02,\n",
      "           2.0633e-02, 2.3288e-01],\n",
      "          [6.0582e-01, 2.9290e-03, 2.3710e-03,  ..., 9.3118e-04,\n",
      "           1.3950e-03, 3.7491e-01],\n",
      "          [5.0584e-01, 2.6114e-02, 8.1106e-03,  ..., 1.5759e-03,\n",
      "           2.4202e-03, 3.0456e-01],\n",
      "          ...,\n",
      "          [3.5332e-01, 5.4281e-03, 4.2366e-03,  ..., 6.2342e-03,\n",
      "           1.1508e-02, 3.6391e-01],\n",
      "          [8.1507e-02, 2.6461e-03, 8.4205e-03,  ..., 2.9634e-02,\n",
      "           3.3698e-02, 1.7861e-01],\n",
      "          [7.9820e-01, 1.8174e-03, 2.7266e-03,  ..., 3.2499e-03,\n",
      "           3.2355e-03, 1.7373e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[1.3504e-01, 8.8583e-03, 9.6187e-03,  ..., 7.0837e-03,\n",
      "           1.1971e-02, 8.1483e-01],\n",
      "          [1.7948e-03, 8.6645e-02, 4.3880e-01,  ..., 2.1691e-01,\n",
      "           9.8781e-02, 1.5606e-01],\n",
      "          [5.0159e-04, 3.0374e-01, 1.5403e-02,  ..., 5.5874e-01,\n",
      "           8.8475e-03, 1.1175e-01],\n",
      "          ...,\n",
      "          [1.5070e-03, 1.1716e-01, 1.2415e-01,  ..., 2.5099e-01,\n",
      "           1.3524e-01, 3.6981e-01],\n",
      "          [2.2615e-03, 1.2373e-01, 5.8018e-03,  ..., 5.3399e-01,\n",
      "           6.7441e-03, 3.2323e-01],\n",
      "          [8.7021e-02, 8.6840e-04, 7.5689e-04,  ..., 9.4826e-04,\n",
      "           7.6710e-03, 8.9995e-01]],\n",
      "\n",
      "         [[8.3967e-02, 1.3216e-01, 1.7061e-01,  ..., 4.4387e-01,\n",
      "           1.5322e-01, 1.1934e-03],\n",
      "          [3.6921e-03, 1.3006e-01, 7.2323e-02,  ..., 9.5887e-02,\n",
      "           3.4695e-01, 2.3636e-01],\n",
      "          [4.7584e-03, 1.8096e-01, 8.4417e-02,  ..., 2.0249e-01,\n",
      "           3.1406e-01, 8.8622e-02],\n",
      "          ...,\n",
      "          [3.0028e-03, 2.3428e-01, 1.0603e-01,  ..., 8.0841e-02,\n",
      "           3.3412e-01, 1.4670e-01],\n",
      "          [2.6036e-03, 9.9779e-02, 1.2942e-01,  ..., 8.4406e-02,\n",
      "           5.7790e-01, 3.5209e-02],\n",
      "          [7.8979e-02, 3.5779e-02, 2.1082e-02,  ..., 1.5191e-02,\n",
      "           2.0991e-02, 6.8575e-01]],\n",
      "\n",
      "         [[5.4901e-02, 1.1018e-01, 9.1524e-02,  ..., 1.1075e-01,\n",
      "           1.3614e-01, 7.0782e-02],\n",
      "          [2.4601e-01, 1.1529e-02, 3.7485e-03,  ..., 2.0248e-02,\n",
      "           1.6932e-02, 6.5019e-01],\n",
      "          [8.1817e-02, 4.1165e-03, 3.9113e-04,  ..., 5.8778e-03,\n",
      "           3.9747e-03, 8.8302e-01],\n",
      "          ...,\n",
      "          [1.1628e-01, 2.7430e-02, 1.2967e-03,  ..., 4.4344e-03,\n",
      "           5.8948e-03, 7.9829e-01],\n",
      "          [2.9220e-01, 4.5479e-03, 2.7054e-03,  ..., 6.1204e-03,\n",
      "           9.1398e-04, 6.2023e-01],\n",
      "          [1.3512e-01, 9.3521e-03, 7.6894e-03,  ..., 1.0541e-02,\n",
      "           1.5486e-02, 7.9614e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[4.2422e-02, 3.9827e-02, 1.2905e-02,  ..., 1.9849e-02,\n",
      "           1.5863e-02, 5.7134e-01],\n",
      "          [1.8158e-01, 2.4543e-02, 5.1056e-02,  ..., 1.9827e-02,\n",
      "           2.5782e-02, 6.4218e-01],\n",
      "          [2.1369e-01, 1.7368e-02, 7.1745e-03,  ..., 8.5889e-03,\n",
      "           6.2583e-02, 6.5846e-01],\n",
      "          ...,\n",
      "          [1.4609e-01, 7.9980e-02, 8.3180e-02,  ..., 3.5688e-02,\n",
      "           6.8529e-02, 5.3238e-01],\n",
      "          [1.4276e-01, 4.4293e-02, 9.8199e-02,  ..., 2.3410e-02,\n",
      "           6.7788e-02, 4.9610e-01],\n",
      "          [3.3506e-02, 2.9223e-03, 1.9491e-03,  ..., 3.2570e-03,\n",
      "           5.5779e-03, 9.4030e-01]],\n",
      "\n",
      "         [[2.0894e-02, 6.0300e-03, 3.8971e-03,  ..., 5.2874e-03,\n",
      "           1.0769e-02, 9.3159e-01],\n",
      "          [2.8512e-02, 8.3760e-03, 1.1474e-02,  ..., 3.8220e-03,\n",
      "           1.4740e-02, 9.2243e-01],\n",
      "          [2.5052e-02, 2.6345e-02, 1.6583e-03,  ..., 2.8679e-03,\n",
      "           4.2546e-03, 9.3119e-01],\n",
      "          ...,\n",
      "          [2.5407e-02, 1.4060e-02, 1.1059e-02,  ..., 1.5958e-02,\n",
      "           1.9758e-02, 7.8556e-01],\n",
      "          [4.0063e-02, 4.1512e-02, 4.5940e-02,  ..., 7.7523e-02,\n",
      "           2.4499e-02, 2.7599e-01],\n",
      "          [1.0312e-02, 1.5471e-03, 1.0323e-03,  ..., 1.0064e-03,\n",
      "           4.2919e-03, 9.7623e-01]],\n",
      "\n",
      "         [[9.3664e-02, 2.7537e-01, 6.9461e-02,  ..., 6.5509e-02,\n",
      "           4.9241e-02, 3.7860e-01],\n",
      "          [2.1798e-02, 4.0821e-03, 3.8932e-01,  ..., 1.1459e-03,\n",
      "           6.2459e-03, 5.6693e-01],\n",
      "          [1.1583e-02, 3.2078e-03, 3.1326e-03,  ..., 2.4275e-03,\n",
      "           6.7313e-04, 9.0420e-01],\n",
      "          ...,\n",
      "          [2.8442e-02, 7.9268e-04, 4.0896e-03,  ..., 2.2330e-02,\n",
      "           4.0401e-02, 8.9784e-01],\n",
      "          [1.1642e-02, 3.3654e-02, 5.0373e-04,  ..., 5.6081e-02,\n",
      "           2.9253e-03, 8.8536e-01],\n",
      "          [1.6880e-02, 5.1349e-03, 6.7710e-03,  ..., 3.8259e-03,\n",
      "           7.7237e-03, 9.2199e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[5.9398e-02, 6.9546e-02, 1.7261e-01,  ..., 2.1731e-02,\n",
      "           5.4832e-02, 5.2959e-01],\n",
      "          [1.9415e-02, 1.1715e-02, 1.2366e-02,  ..., 1.9919e-03,\n",
      "           2.5419e-03, 6.9220e-01],\n",
      "          [1.6870e-02, 1.8033e-02, 1.2501e-02,  ..., 3.0209e-03,\n",
      "           2.8798e-03, 7.8642e-01],\n",
      "          ...,\n",
      "          [3.0990e-02, 1.0635e-03, 1.2639e-03,  ..., 5.6890e-03,\n",
      "           2.5032e-02, 9.1168e-01],\n",
      "          [5.0636e-02, 6.5443e-03, 7.0761e-03,  ..., 5.6270e-03,\n",
      "           2.1661e-02, 8.8659e-01],\n",
      "          [2.2239e-02, 8.2491e-03, 8.2056e-03,  ..., 5.4488e-03,\n",
      "           7.2593e-03, 9.2796e-01]],\n",
      "\n",
      "         [[3.2678e-02, 2.0939e-03, 1.8213e-02,  ..., 4.1502e-03,\n",
      "           1.5985e-02, 8.6971e-01],\n",
      "          [2.5147e-03, 1.0917e-03, 1.5327e-03,  ..., 1.6466e-04,\n",
      "           1.0098e-03, 9.8580e-01],\n",
      "          [6.4620e-02, 2.1578e-02, 9.3014e-04,  ..., 7.3639e-04,\n",
      "           1.2868e-03, 8.9591e-01],\n",
      "          ...,\n",
      "          [3.4418e-03, 6.3645e-04, 4.5552e-04,  ..., 4.1681e-03,\n",
      "           5.7229e-03, 9.7873e-01],\n",
      "          [9.4536e-02, 3.0420e-03, 1.5148e-03,  ..., 2.4495e-02,\n",
      "           6.2256e-03, 8.5973e-01],\n",
      "          [7.7947e-03, 4.4958e-04, 1.0288e-03,  ..., 3.5537e-04,\n",
      "           1.3839e-03, 9.7875e-01]],\n",
      "\n",
      "         [[5.6800e-02, 3.4250e-02, 8.1703e-02,  ..., 4.8937e-02,\n",
      "           1.3334e-01, 5.3062e-01],\n",
      "          [2.4785e-02, 3.2267e-03, 2.9831e-03,  ..., 4.4580e-03,\n",
      "           6.5348e-03, 8.3476e-01],\n",
      "          [1.9823e-02, 7.0889e-04, 5.0618e-04,  ..., 9.7769e-04,\n",
      "           4.0195e-03, 8.3102e-01],\n",
      "          ...,\n",
      "          [1.3746e-02, 5.0424e-04, 3.4046e-04,  ..., 6.7938e-04,\n",
      "           1.5233e-03, 9.2501e-01],\n",
      "          [2.7390e-02, 3.6394e-03, 3.2456e-03,  ..., 3.3904e-03,\n",
      "           1.2630e-02, 8.1108e-01],\n",
      "          [3.5771e-02, 2.9038e-03, 7.6655e-04,  ..., 5.6730e-04,\n",
      "           1.2561e-03, 9.5362e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[4.4577e-02, 7.9799e-02, 1.1756e-01,  ..., 1.6535e-02,\n",
      "           5.0641e-02, 6.5757e-01],\n",
      "          [1.5336e-03, 1.3471e-02, 5.7169e-01,  ..., 1.4983e-03,\n",
      "           1.9250e-02, 3.8409e-01],\n",
      "          [4.9807e-02, 9.8418e-04, 1.4620e-03,  ..., 9.7862e-05,\n",
      "           1.9587e-03, 6.8637e-01],\n",
      "          ...,\n",
      "          [9.9667e-03, 2.3011e-03, 1.3850e-02,  ..., 2.2898e-02,\n",
      "           5.0380e-01, 4.4269e-01],\n",
      "          [1.1051e-02, 9.8634e-05, 1.5187e-04,  ..., 8.6872e-04,\n",
      "           1.2361e-03, 9.8520e-01],\n",
      "          [2.7071e-01, 6.4147e-03, 1.1738e-02,  ..., 8.4076e-03,\n",
      "           5.6827e-02, 6.1372e-01]],\n",
      "\n",
      "         [[2.3136e-02, 4.5039e-02, 8.0630e-02,  ..., 5.9362e-02,\n",
      "           1.6558e-01, 5.3928e-01],\n",
      "          [1.8645e-01, 7.6194e-03, 6.3292e-03,  ..., 5.3958e-04,\n",
      "           1.0912e-02, 7.7730e-01],\n",
      "          [2.4387e-01, 4.5039e-02, 1.2052e-02,  ..., 1.6983e-03,\n",
      "           1.7204e-02, 6.4255e-01],\n",
      "          ...,\n",
      "          [4.9391e-02, 4.0477e-03, 4.1912e-03,  ..., 7.7002e-03,\n",
      "           3.6139e-02, 7.8812e-01],\n",
      "          [5.6009e-02, 8.6477e-03, 5.6805e-03,  ..., 5.4537e-02,\n",
      "           2.6654e-02, 7.0904e-01],\n",
      "          [2.0237e-02, 9.8626e-03, 4.3641e-03,  ..., 6.9329e-03,\n",
      "           1.9646e-02, 9.1203e-01]],\n",
      "\n",
      "         [[1.8730e-02, 8.3062e-04, 1.0463e-03,  ..., 4.9721e-04,\n",
      "           8.2127e-03, 5.5478e-01],\n",
      "          [1.7323e-02, 5.6497e-03, 1.2710e-02,  ..., 1.5855e-02,\n",
      "           4.2379e-02, 8.8305e-01],\n",
      "          [1.4270e-02, 1.8446e-02, 9.1110e-03,  ..., 2.0589e-02,\n",
      "           4.1415e-02, 7.6254e-01],\n",
      "          ...,\n",
      "          [5.8155e-03, 2.3761e-03, 3.0606e-03,  ..., 1.8551e-02,\n",
      "           3.4390e-02, 9.3136e-01],\n",
      "          [3.3150e-02, 1.1295e-02, 3.6457e-03,  ..., 2.4227e-02,\n",
      "           1.6849e-02, 8.8138e-01],\n",
      "          [1.2965e-02, 3.5340e-03, 5.4588e-03,  ..., 5.3433e-03,\n",
      "           1.1667e-02, 9.3620e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[3.0436e-03, 2.3966e-02, 2.8033e-01,  ..., 1.8720e-02,\n",
      "           1.6991e-01, 3.8467e-02],\n",
      "          [1.0640e-03, 8.0568e-03, 4.9515e-03,  ..., 1.3005e-03,\n",
      "           7.1140e-04, 9.8371e-01],\n",
      "          [1.4256e-03, 5.4302e-03, 8.0341e-03,  ..., 1.2958e-03,\n",
      "           2.0472e-03, 9.8107e-01],\n",
      "          ...,\n",
      "          [3.7374e-03, 4.4360e-02, 2.9376e-02,  ..., 7.2877e-03,\n",
      "           5.0575e-03, 9.0905e-01],\n",
      "          [4.6800e-03, 2.5880e-02, 3.8890e-02,  ..., 3.1652e-03,\n",
      "           1.6836e-02, 8.9865e-01],\n",
      "          [4.2736e-03, 2.1773e-03, 5.0415e-03,  ..., 9.9372e-04,\n",
      "           5.5395e-03, 9.7422e-01]],\n",
      "\n",
      "         [[7.7862e-02, 5.2833e-03, 1.5543e-02,  ..., 5.1737e-03,\n",
      "           1.1415e-01, 7.4769e-02],\n",
      "          [3.1024e-02, 1.4528e-03, 4.0589e-03,  ..., 1.8918e-03,\n",
      "           2.8300e-02, 8.6272e-01],\n",
      "          [3.5301e-02, 7.3034e-03, 9.9518e-03,  ..., 5.9838e-03,\n",
      "           3.2133e-02, 8.0604e-01],\n",
      "          ...,\n",
      "          [2.4886e-02, 2.3992e-03, 5.7511e-03,  ..., 1.6642e-03,\n",
      "           8.0092e-03, 8.2269e-01],\n",
      "          [9.4973e-02, 2.2235e-02, 4.7009e-02,  ..., 6.6630e-03,\n",
      "           4.2804e-02, 5.4211e-01],\n",
      "          [1.2294e-02, 2.8619e-03, 4.4197e-03,  ..., 2.4666e-03,\n",
      "           9.8198e-03, 9.5316e-01]],\n",
      "\n",
      "         [[1.2553e-02, 9.8191e-03, 6.4173e-02,  ..., 7.4321e-03,\n",
      "           4.6639e-01, 3.0913e-01],\n",
      "          [1.3959e-02, 3.6955e-03, 1.6796e-02,  ..., 1.4611e-03,\n",
      "           1.7991e-02, 9.3988e-01],\n",
      "          [1.3525e-02, 1.2315e-03, 5.7692e-03,  ..., 3.7451e-04,\n",
      "           6.1932e-03, 9.5902e-01],\n",
      "          ...,\n",
      "          [2.8920e-02, 5.3003e-03, 1.5832e-02,  ..., 4.8550e-03,\n",
      "           5.8401e-02, 7.8783e-01],\n",
      "          [6.1049e-02, 4.5489e-03, 9.8863e-03,  ..., 5.2163e-03,\n",
      "           2.5777e-02, 6.4513e-01],\n",
      "          [2.3981e-02, 3.1593e-03, 6.5964e-03,  ..., 2.4466e-03,\n",
      "           2.1011e-02, 9.1165e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[4.7118e-02, 1.0532e-02, 6.2810e-03,  ..., 2.7222e-03,\n",
      "           1.9283e-02, 8.1983e-01],\n",
      "          [4.5622e-01, 2.1336e-02, 3.0967e-02,  ..., 1.5271e-03,\n",
      "           4.5196e-02, 3.7960e-01],\n",
      "          [2.1814e-01, 7.2573e-02, 3.5893e-02,  ..., 2.0247e-03,\n",
      "           2.5163e-02, 5.7834e-01],\n",
      "          ...,\n",
      "          [7.3651e-02, 1.9162e-02, 1.1931e-02,  ..., 2.5672e-02,\n",
      "           8.0664e-02, 1.6794e-01],\n",
      "          [3.3993e-02, 8.5647e-03, 1.1483e-02,  ..., 2.1956e-02,\n",
      "           7.7142e-02, 3.3617e-01],\n",
      "          [1.9467e-02, 3.9699e-03, 3.1363e-03,  ..., 4.0585e-03,\n",
      "           2.3151e-02, 8.9479e-01]],\n",
      "\n",
      "         [[7.0445e-03, 1.2195e-03, 9.0479e-03,  ..., 4.5399e-03,\n",
      "           5.5399e-03, 8.9052e-01],\n",
      "          [3.3163e-04, 1.7573e-03, 2.5768e-03,  ..., 1.9544e-03,\n",
      "           4.6645e-03, 9.8521e-01],\n",
      "          [1.6277e-03, 4.1049e-03, 1.2632e-02,  ..., 1.0414e-03,\n",
      "           4.1447e-03, 9.6464e-01],\n",
      "          ...,\n",
      "          [5.3395e-03, 2.4587e-02, 5.2501e-02,  ..., 4.6258e-02,\n",
      "           3.5431e-02, 7.6578e-01],\n",
      "          [7.4678e-03, 3.6927e-03, 4.1792e-03,  ..., 6.7755e-03,\n",
      "           1.1518e-03, 9.4506e-01],\n",
      "          [9.7709e-03, 4.6870e-03, 9.8747e-03,  ..., 6.6155e-03,\n",
      "           1.6207e-02, 9.1553e-01]],\n",
      "\n",
      "         [[6.8967e-03, 1.9477e-02, 1.6712e-01,  ..., 5.6579e-03,\n",
      "           1.6009e-01, 5.4827e-01],\n",
      "          [1.2659e-01, 4.7557e-04, 1.5810e-03,  ..., 7.4271e-05,\n",
      "           3.5071e-02, 8.3266e-01],\n",
      "          [2.6124e-02, 2.6217e-01, 1.1135e-02,  ..., 4.2675e-03,\n",
      "           3.8513e-03, 6.8956e-01],\n",
      "          ...,\n",
      "          [6.2288e-02, 1.2559e-04, 6.6943e-04,  ..., 2.0350e-03,\n",
      "           1.8105e-01, 5.7614e-01],\n",
      "          [2.7695e-02, 6.0293e-03, 3.5578e-03,  ..., 2.0396e-01,\n",
      "           2.7883e-02, 6.9349e-01],\n",
      "          [5.3371e-03, 8.8784e-03, 1.0340e-02,  ..., 1.3270e-02,\n",
      "           1.2671e-02, 9.2260e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[3.2048e-04, 1.5348e-03, 1.7055e-02,  ..., 4.1098e-03,\n",
      "           1.9344e-02, 9.3404e-01],\n",
      "          [2.9955e-02, 1.4647e-02, 3.9235e-02,  ..., 1.0425e-02,\n",
      "           5.1723e-02, 6.3216e-01],\n",
      "          [1.8943e-02, 1.7355e-02, 2.1525e-02,  ..., 6.4969e-03,\n",
      "           3.3122e-02, 7.5079e-01],\n",
      "          ...,\n",
      "          [1.5823e-02, 1.2603e-02, 3.7301e-02,  ..., 1.6457e-02,\n",
      "           6.0163e-02, 2.2935e-01],\n",
      "          [5.4388e-03, 8.3172e-03, 2.4085e-02,  ..., 1.2710e-02,\n",
      "           2.8675e-02, 5.6645e-01],\n",
      "          [1.2096e-02, 7.9429e-04, 2.3637e-03,  ..., 1.5789e-03,\n",
      "           1.6084e-02, 9.4416e-01]],\n",
      "\n",
      "         [[6.2376e-02, 1.2861e-02, 1.7250e-02,  ..., 7.4658e-03,\n",
      "           2.1172e-01, 1.6505e-01],\n",
      "          [2.6603e-03, 5.6033e-03, 7.7503e-03,  ..., 5.4221e-03,\n",
      "           1.4706e-02, 9.1819e-01],\n",
      "          [8.4078e-03, 1.1440e-02, 9.5270e-03,  ..., 1.0951e-02,\n",
      "           2.0517e-02, 7.8484e-01],\n",
      "          ...,\n",
      "          [2.1314e-03, 1.3277e-03, 1.8166e-03,  ..., 4.2224e-03,\n",
      "           1.8388e-02, 9.6234e-01],\n",
      "          [1.5301e-02, 5.0227e-03, 9.6778e-03,  ..., 6.1414e-03,\n",
      "           5.4662e-02, 8.5680e-01],\n",
      "          [4.4162e-03, 2.6193e-03, 2.2859e-03,  ..., 4.2231e-03,\n",
      "           1.0424e-02, 9.6636e-01]],\n",
      "\n",
      "         [[5.8483e-02, 9.5628e-02, 3.2214e-02,  ..., 2.9174e-02,\n",
      "           6.3613e-02, 6.6455e-01],\n",
      "          [1.0688e-02, 7.7250e-03, 9.3853e-02,  ..., 2.0724e-03,\n",
      "           7.2353e-03, 5.6894e-01],\n",
      "          [1.1786e-02, 1.1954e-02, 5.8957e-03,  ..., 1.4999e-03,\n",
      "           2.2028e-03, 4.5123e-01],\n",
      "          ...,\n",
      "          [8.2247e-03, 2.5184e-03, 2.8281e-03,  ..., 4.6763e-03,\n",
      "           3.5181e-02, 8.5645e-01],\n",
      "          [9.1214e-03, 6.8423e-03, 1.5239e-03,  ..., 7.3463e-03,\n",
      "           1.7525e-02, 9.2927e-01],\n",
      "          [1.9924e-02, 9.4724e-03, 9.8599e-03,  ..., 7.7487e-03,\n",
      "           1.5886e-02, 8.9716e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[7.2796e-03, 4.8440e-04, 6.7570e-04,  ..., 4.5665e-04,\n",
      "           4.7260e-03, 9.7439e-01],\n",
      "          [1.1327e-02, 9.8596e-03, 1.1181e-02,  ..., 1.0792e-03,\n",
      "           6.9807e-03, 9.1354e-01],\n",
      "          [2.9909e-02, 4.9994e-03, 8.7343e-03,  ..., 4.9919e-04,\n",
      "           1.6874e-02, 8.8511e-01],\n",
      "          ...,\n",
      "          [3.3963e-02, 7.1787e-03, 8.8748e-03,  ..., 9.7445e-03,\n",
      "           7.2704e-02, 7.1175e-01],\n",
      "          [5.0688e-02, 5.8932e-03, 4.4605e-03,  ..., 2.3904e-02,\n",
      "           2.6174e-02, 7.0789e-01],\n",
      "          [1.6990e-02, 4.4442e-03, 4.6856e-03,  ..., 4.4390e-03,\n",
      "           2.2897e-02, 9.1206e-01]],\n",
      "\n",
      "         [[1.1937e-02, 1.3111e-02, 3.1072e-01,  ..., 2.8623e-02,\n",
      "           1.0131e-01, 4.5905e-01],\n",
      "          [2.1699e-03, 6.9610e-03, 1.5081e-01,  ..., 7.0934e-03,\n",
      "           1.4702e-02, 8.0951e-01],\n",
      "          [3.1595e-03, 2.8876e-02, 6.0235e-02,  ..., 1.3609e-02,\n",
      "           8.6296e-03, 8.6014e-01],\n",
      "          ...,\n",
      "          [4.8719e-03, 2.4370e-03, 1.8186e-02,  ..., 9.6545e-03,\n",
      "           4.7080e-02, 9.1172e-01],\n",
      "          [4.6516e-03, 5.7157e-03, 2.2346e-02,  ..., 1.4154e-02,\n",
      "           9.1400e-03, 9.2667e-01],\n",
      "          [1.2555e-02, 9.8442e-03, 1.3206e-02,  ..., 7.6167e-03,\n",
      "           3.0111e-02, 9.1143e-01]],\n",
      "\n",
      "         [[1.5211e-02, 3.8087e-03, 2.7465e-02,  ..., 4.1055e-03,\n",
      "           1.3230e-01, 7.4217e-01],\n",
      "          [7.9568e-03, 2.4081e-03, 4.2205e-03,  ..., 4.1643e-04,\n",
      "           2.0749e-02, 9.5468e-01],\n",
      "          [2.5112e-03, 2.4362e-02, 4.0101e-03,  ..., 4.0284e-03,\n",
      "           4.5496e-04, 9.5804e-01],\n",
      "          ...,\n",
      "          [5.0223e-03, 6.5422e-04, 6.4075e-03,  ..., 3.5394e-03,\n",
      "           2.4462e-01, 7.2221e-01],\n",
      "          [3.9207e-03, 3.7521e-03, 1.3115e-03,  ..., 3.6610e-02,\n",
      "           2.1569e-03, 9.4262e-01],\n",
      "          [1.1366e-02, 4.3955e-03, 5.7982e-03,  ..., 4.6074e-03,\n",
      "           2.3023e-02, 9.4096e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[6.9473e-02, 7.8216e-03, 1.6208e-02,  ..., 2.1218e-02,\n",
      "           1.7177e-01, 4.7220e-01],\n",
      "          [8.9727e-02, 3.6437e-03, 3.3735e-03,  ..., 2.0070e-03,\n",
      "           7.4049e-03, 8.9063e-01],\n",
      "          [1.4738e-01, 3.8679e-03, 5.0968e-03,  ..., 3.5996e-03,\n",
      "           7.2263e-03, 8.2930e-01],\n",
      "          ...,\n",
      "          [3.4733e-02, 1.0717e-03, 6.5157e-04,  ..., 5.3958e-03,\n",
      "           1.8515e-02, 9.3374e-01],\n",
      "          [6.7989e-02, 1.1244e-03, 1.0204e-03,  ..., 4.0238e-03,\n",
      "           6.3729e-03, 9.0877e-01],\n",
      "          [3.2237e-02, 5.6524e-03, 5.8062e-03,  ..., 6.8684e-03,\n",
      "           1.1404e-02, 9.2815e-01]],\n",
      "\n",
      "         [[4.4562e-02, 4.8411e-02, 6.7811e-02,  ..., 3.9827e-02,\n",
      "           1.0704e-01, 4.4266e-01],\n",
      "          [1.1649e-01, 6.8192e-02, 3.9175e-02,  ..., 9.7283e-02,\n",
      "           1.0798e-01, 5.1798e-01],\n",
      "          [9.7296e-02, 4.3595e-02, 2.5700e-02,  ..., 5.5769e-02,\n",
      "           9.6717e-02, 6.2794e-01],\n",
      "          ...,\n",
      "          [1.3444e-01, 4.9042e-02, 3.2381e-02,  ..., 7.2857e-02,\n",
      "           2.2805e-01, 4.4824e-01],\n",
      "          [7.6005e-02, 3.0749e-02, 4.1090e-02,  ..., 5.1692e-02,\n",
      "           1.1328e-01, 6.5986e-01],\n",
      "          [2.9729e-02, 2.2711e-02, 1.4063e-02,  ..., 1.7750e-02,\n",
      "           5.0797e-02, 8.1753e-01]],\n",
      "\n",
      "         [[1.9930e-02, 5.5491e-02, 1.0577e-02,  ..., 5.3441e-02,\n",
      "           2.3280e-02, 8.1694e-01],\n",
      "          [2.6942e-03, 3.9535e-04, 2.7585e-02,  ..., 2.5187e-04,\n",
      "           3.3105e-02, 9.3325e-01],\n",
      "          [6.5297e-03, 5.2667e-04, 1.7254e-04,  ..., 3.3266e-04,\n",
      "           1.4184e-03, 9.6527e-01],\n",
      "          ...,\n",
      "          [8.9698e-04, 2.6265e-04, 1.9352e-03,  ..., 8.2531e-04,\n",
      "           5.5703e-02, 9.3946e-01],\n",
      "          [6.3677e-03, 3.8933e-03, 2.5117e-04,  ..., 8.6712e-03,\n",
      "           2.8066e-03, 9.7136e-01],\n",
      "          [1.5157e-02, 1.1246e-02, 1.0508e-02,  ..., 6.4517e-03,\n",
      "           1.3602e-02, 9.2586e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[8.1155e-03, 2.8974e-03, 9.2901e-03,  ..., 6.8406e-03,\n",
      "           3.9204e-02, 8.4413e-01],\n",
      "          [7.2132e-02, 4.5701e-02, 6.3237e-02,  ..., 1.8483e-02,\n",
      "           3.7167e-02, 2.0938e-01],\n",
      "          [4.5994e-02, 3.3940e-02, 1.7397e-02,  ..., 1.9023e-02,\n",
      "           2.3631e-02, 1.5771e-01],\n",
      "          ...,\n",
      "          [2.2557e-02, 2.3531e-02, 2.9437e-02,  ..., 1.4363e-02,\n",
      "           3.0458e-02, 6.8765e-01],\n",
      "          [5.2646e-03, 5.8881e-03, 4.5577e-03,  ..., 7.3402e-03,\n",
      "           1.3954e-02, 9.1226e-01],\n",
      "          [1.5016e-02, 7.0473e-03, 9.5938e-03,  ..., 5.4708e-03,\n",
      "           1.6327e-02, 9.2476e-01]],\n",
      "\n",
      "         [[2.0322e-02, 2.8160e-03, 1.2281e-03,  ..., 8.0060e-03,\n",
      "           1.5642e-02, 9.4061e-01],\n",
      "          [1.0671e-03, 1.4012e-02, 5.6920e-03,  ..., 1.2619e-02,\n",
      "           2.1900e-02, 9.3410e-01],\n",
      "          [3.6205e-03, 1.9336e-02, 8.9413e-03,  ..., 1.3777e-02,\n",
      "           2.9521e-02, 8.9533e-01],\n",
      "          ...,\n",
      "          [2.0653e-03, 1.7622e-02, 2.1550e-02,  ..., 1.2493e-02,\n",
      "           2.2872e-02, 9.2041e-01],\n",
      "          [2.9658e-03, 1.1513e-02, 1.7070e-02,  ..., 7.8508e-03,\n",
      "           2.4444e-02, 9.2853e-01],\n",
      "          [2.3743e-03, 4.3111e-03, 4.1668e-03,  ..., 4.8600e-03,\n",
      "           1.0916e-02, 9.6692e-01]],\n",
      "\n",
      "         [[1.1766e-02, 1.5463e-03, 1.5489e-03,  ..., 3.5179e-03,\n",
      "           2.1214e-02, 9.4757e-01],\n",
      "          [1.1639e-02, 2.6095e-02, 1.2420e-01,  ..., 2.0498e-02,\n",
      "           6.5244e-02, 5.8350e-01],\n",
      "          [2.9069e-02, 1.0801e-02, 1.4348e-02,  ..., 7.7274e-03,\n",
      "           1.3070e-02, 7.0629e-01],\n",
      "          ...,\n",
      "          [2.5721e-02, 8.2693e-03, 5.5863e-02,  ..., 1.4475e-02,\n",
      "           1.1793e-01, 6.0288e-01],\n",
      "          [2.0289e-02, 7.4911e-03, 2.6396e-02,  ..., 2.0898e-02,\n",
      "           4.3470e-02, 7.5651e-01],\n",
      "          [1.5644e-02, 8.7578e-03, 1.3571e-02,  ..., 1.3625e-02,\n",
      "           1.8085e-02, 9.0442e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[0.0201, 0.0200, 0.0386,  ..., 0.0260, 0.0692, 0.6120],\n",
      "          [0.0188, 0.1410, 0.1055,  ..., 0.0416, 0.1506, 0.5399],\n",
      "          [0.0112, 0.0943, 0.2264,  ..., 0.0275, 0.1395, 0.4983],\n",
      "          ...,\n",
      "          [0.0161, 0.0620, 0.0569,  ..., 0.1123, 0.1416, 0.6087],\n",
      "          [0.0157, 0.0774, 0.1727,  ..., 0.0589, 0.1718, 0.4923],\n",
      "          [0.0077, 0.0392, 0.0472,  ..., 0.0313, 0.0270, 0.8061]],\n",
      "\n",
      "         [[0.0400, 0.0055, 0.0134,  ..., 0.0207, 0.0786, 0.7366],\n",
      "          [0.0455, 0.0133, 0.0174,  ..., 0.0119, 0.0108, 0.8983],\n",
      "          [0.0694, 0.0216, 0.0097,  ..., 0.0100, 0.0131, 0.8741],\n",
      "          ...,\n",
      "          [0.0597, 0.0134, 0.0115,  ..., 0.0094, 0.0113, 0.8913],\n",
      "          [0.1056, 0.0112, 0.0169,  ..., 0.0074, 0.0131, 0.8384],\n",
      "          [0.1006, 0.0225, 0.0476,  ..., 0.0265, 0.0497, 0.7008]],\n",
      "\n",
      "         [[0.0880, 0.0033, 0.0032,  ..., 0.0056, 0.0080, 0.8299],\n",
      "          [0.0115, 0.0516, 0.1058,  ..., 0.0982, 0.0521, 0.5680],\n",
      "          [0.0152, 0.0397, 0.0507,  ..., 0.0625, 0.0302, 0.7377],\n",
      "          ...,\n",
      "          [0.0153, 0.0261, 0.0557,  ..., 0.0271, 0.0390, 0.7009],\n",
      "          [0.0104, 0.0292, 0.0381,  ..., 0.0187, 0.0141, 0.7586],\n",
      "          [0.0185, 0.0755, 0.0623,  ..., 0.0700, 0.0386, 0.5012]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0227, 0.0069, 0.0053,  ..., 0.0116, 0.0219, 0.9214],\n",
      "          [0.0128, 0.0031, 0.0093,  ..., 0.0015, 0.0089, 0.9492],\n",
      "          [0.0122, 0.2986, 0.0088,  ..., 0.0417, 0.0329, 0.5850],\n",
      "          ...,\n",
      "          [0.0107, 0.0110, 0.0073,  ..., 0.0068, 0.0312, 0.9221],\n",
      "          [0.0091, 0.0808, 0.0094,  ..., 0.0687, 0.0467, 0.7710],\n",
      "          [0.0588, 0.0258, 0.0333,  ..., 0.0244, 0.0574, 0.7455]],\n",
      "\n",
      "         [[0.0325, 0.0426, 0.0333,  ..., 0.1133, 0.0683, 0.4964],\n",
      "          [0.0266, 0.0579, 0.0277,  ..., 0.0920, 0.0173, 0.7599],\n",
      "          [0.0623, 0.0434, 0.0113,  ..., 0.0475, 0.0102, 0.8090],\n",
      "          ...,\n",
      "          [0.0284, 0.0243, 0.0149,  ..., 0.0322, 0.0191, 0.8585],\n",
      "          [0.0355, 0.0117, 0.0097,  ..., 0.0314, 0.0077, 0.8810],\n",
      "          [0.0195, 0.1578, 0.0742,  ..., 0.2348, 0.0393, 0.3864]],\n",
      "\n",
      "         [[0.0985, 0.0587, 0.0872,  ..., 0.0460, 0.0867, 0.1800],\n",
      "          [0.1020, 0.0081, 0.0227,  ..., 0.0130, 0.0374, 0.7671],\n",
      "          [0.0940, 0.0218, 0.0104,  ..., 0.0248, 0.0384, 0.7776],\n",
      "          ...,\n",
      "          [0.1056, 0.0214, 0.0215,  ..., 0.0036, 0.0184, 0.8034],\n",
      "          [0.0478, 0.0306, 0.0275,  ..., 0.0117, 0.0100, 0.8230],\n",
      "          [0.1261, 0.0992, 0.0938,  ..., 0.1074, 0.1436, 0.1219]]]],\n",
      "       grad_fn=<SoftmaxBackward0>), tensor([[[[2.2630e-01, 3.7651e-03, 5.8672e-03,  ..., 4.8615e-03,\n",
      "           2.5475e-02, 4.2482e-02],\n",
      "          [1.1852e-01, 1.5820e-02, 3.9001e-02,  ..., 7.9261e-02,\n",
      "           5.7988e-02, 2.9421e-01],\n",
      "          [4.8723e-02, 2.0490e-02, 5.2125e-02,  ..., 7.2282e-02,\n",
      "           3.6645e-02, 2.7517e-01],\n",
      "          ...,\n",
      "          [4.1683e-02, 1.0522e-02, 2.7664e-02,  ..., 6.6017e-02,\n",
      "           7.3056e-02, 4.5426e-01],\n",
      "          [6.3301e-02, 2.6748e-02, 6.0287e-02,  ..., 5.1800e-02,\n",
      "           9.8396e-02, 4.3775e-01],\n",
      "          [4.3202e-02, 6.9188e-02, 6.5163e-02,  ..., 7.6943e-02,\n",
      "           3.4775e-02, 5.5009e-01]],\n",
      "\n",
      "         [[4.5551e-02, 4.1585e-03, 8.2729e-03,  ..., 8.6265e-03,\n",
      "           1.9850e-02, 2.5782e-01],\n",
      "          [3.0657e-04, 1.3605e-02, 4.5169e-03,  ..., 2.1335e-03,\n",
      "           4.7485e-03, 9.7393e-01],\n",
      "          [4.8504e-04, 7.1914e-03, 1.3692e-01,  ..., 2.6300e-03,\n",
      "           3.3839e-02, 8.1777e-01],\n",
      "          ...,\n",
      "          [6.9583e-04, 4.2168e-03, 6.3857e-03,  ..., 4.6647e-03,\n",
      "           7.0787e-03, 9.7532e-01],\n",
      "          [1.2373e-03, 1.0977e-02, 4.2247e-02,  ..., 3.8910e-03,\n",
      "           3.3333e-02, 9.0244e-01],\n",
      "          [2.0176e-03, 3.5299e-03, 2.6023e-03,  ..., 2.5023e-03,\n",
      "           3.4942e-03, 9.8334e-01]],\n",
      "\n",
      "         [[1.1430e-02, 7.3553e-03, 2.9416e-02,  ..., 3.9260e-03,\n",
      "           8.5469e-03, 2.3991e-01],\n",
      "          [4.2369e-02, 1.4883e-02, 1.8609e-02,  ..., 1.3077e-02,\n",
      "           1.6103e-02, 8.4386e-01],\n",
      "          [7.9573e-02, 1.2983e-02, 2.0155e-02,  ..., 1.5043e-02,\n",
      "           2.4080e-02, 3.3242e-01],\n",
      "          ...,\n",
      "          [7.4177e-02, 1.5449e-02, 3.1769e-02,  ..., 1.3878e-02,\n",
      "           3.0046e-02, 6.7723e-01],\n",
      "          [4.4801e-02, 6.3549e-03, 1.4809e-02,  ..., 6.2275e-03,\n",
      "           2.1766e-02, 8.1834e-01],\n",
      "          [2.2312e-02, 1.3917e-02, 2.8228e-02,  ..., 9.9731e-03,\n",
      "           2.0387e-02, 8.7241e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[8.4025e-03, 1.4182e-02, 2.1937e-02,  ..., 6.9089e-03,\n",
      "           1.2035e-02, 8.8695e-01],\n",
      "          [1.9007e-03, 9.2349e-02, 1.4317e-02,  ..., 1.2128e-02,\n",
      "           1.1572e-02, 8.6375e-01],\n",
      "          [5.0697e-04, 1.1483e-02, 1.8321e-01,  ..., 3.0958e-03,\n",
      "           4.8338e-02, 7.4351e-01],\n",
      "          ...,\n",
      "          [1.4932e-03, 1.1019e-02, 5.1174e-03,  ..., 2.2624e-02,\n",
      "           1.5333e-02, 9.3884e-01],\n",
      "          [8.2048e-04, 1.5760e-02, 3.8369e-02,  ..., 8.1555e-03,\n",
      "           1.3169e-01, 7.7694e-01],\n",
      "          [6.9229e-04, 9.2866e-03, 7.6210e-03,  ..., 4.3929e-03,\n",
      "           4.4502e-03, 9.6798e-01]],\n",
      "\n",
      "         [[8.3293e-03, 2.8011e-02, 4.0922e-02,  ..., 3.4181e-02,\n",
      "           5.7285e-02, 3.2066e-01],\n",
      "          [9.2635e-01, 2.2354e-02, 1.3080e-02,  ..., 5.1028e-03,\n",
      "           1.5021e-02, 1.7439e-02],\n",
      "          [8.8857e-01, 1.1051e-02, 5.2206e-02,  ..., 5.8473e-03,\n",
      "           2.0482e-02, 2.1292e-02],\n",
      "          ...,\n",
      "          [8.8636e-01, 8.2883e-03, 4.6543e-03,  ..., 6.2264e-02,\n",
      "           3.1196e-02, 6.9191e-03],\n",
      "          [6.5675e-01, 1.8478e-02, 1.7071e-02,  ..., 3.6004e-02,\n",
      "           2.3220e-01, 3.7115e-02],\n",
      "          [6.8328e-02, 1.0567e-01, 1.0576e-01,  ..., 1.0090e-01,\n",
      "           1.1022e-01, 1.5588e-01]],\n",
      "\n",
      "         [[2.1073e-02, 1.4401e-01, 1.9020e-01,  ..., 8.1998e-02,\n",
      "           6.6116e-02, 3.5191e-01],\n",
      "          [9.2356e-04, 4.8841e-03, 7.6541e-03,  ..., 6.1276e-03,\n",
      "           4.3269e-03, 9.0558e-01],\n",
      "          [1.0800e-03, 3.0319e-02, 3.2716e-02,  ..., 2.2958e-02,\n",
      "           1.6082e-02, 7.2792e-01],\n",
      "          ...,\n",
      "          [6.9445e-04, 1.6355e-02, 3.6703e-02,  ..., 1.7743e-02,\n",
      "           1.9881e-02, 7.5347e-01],\n",
      "          [9.7452e-04, 1.4837e-02, 3.3567e-02,  ..., 3.6295e-02,\n",
      "           1.2891e-02, 6.9944e-01],\n",
      "          [8.0701e-02, 7.0628e-03, 7.2643e-03,  ..., 5.2202e-03,\n",
      "           6.4878e-03, 8.6547e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[6.8206e-02, 4.3392e-02, 3.3463e-02,  ..., 4.1136e-02,\n",
      "           2.7454e-02, 6.9031e-01],\n",
      "          [7.0250e-03, 9.8534e-03, 5.8671e-03,  ..., 5.8446e-03,\n",
      "           3.4524e-03, 9.6286e-01],\n",
      "          [1.2880e-02, 2.0252e-02, 2.7758e-02,  ..., 1.2147e-02,\n",
      "           1.1512e-02, 9.0273e-01],\n",
      "          ...,\n",
      "          [7.4909e-03, 3.6975e-02, 4.2653e-02,  ..., 2.4126e-02,\n",
      "           3.0302e-02, 8.3806e-01],\n",
      "          [1.0656e-02, 4.0313e-02, 8.0322e-02,  ..., 3.4067e-02,\n",
      "           2.5939e-02, 7.7874e-01],\n",
      "          [2.7602e-03, 2.5506e-03, 1.9986e-03,  ..., 3.0346e-03,\n",
      "           3.0270e-03, 9.8070e-01]],\n",
      "\n",
      "         [[1.4630e-01, 1.9841e-01, 1.8357e-01,  ..., 7.3171e-02,\n",
      "           5.0343e-02, 2.9741e-01],\n",
      "          [2.2631e-02, 2.3667e-02, 2.2661e-02,  ..., 1.0149e-02,\n",
      "           1.5846e-02, 8.3396e-01],\n",
      "          [3.4155e-02, 4.5750e-02, 3.2251e-02,  ..., 2.5106e-02,\n",
      "           2.4404e-02, 7.3904e-01],\n",
      "          ...,\n",
      "          [1.2611e-02, 1.6034e-02, 1.5314e-02,  ..., 8.1722e-03,\n",
      "           3.6081e-02, 8.2406e-01],\n",
      "          [2.4974e-02, 3.5483e-02, 3.2695e-02,  ..., 2.7667e-02,\n",
      "           3.5391e-02, 7.3199e-01],\n",
      "          [5.4638e-03, 7.4304e-03, 5.2409e-03,  ..., 6.4220e-03,\n",
      "           9.4496e-03, 9.3808e-01]],\n",
      "\n",
      "         [[3.9314e-02, 3.8176e-02, 8.3902e-02,  ..., 2.7169e-02,\n",
      "           4.9844e-02, 3.7447e-02],\n",
      "          [3.5819e-02, 2.0768e-01, 1.5143e-01,  ..., 1.2069e-01,\n",
      "           9.6611e-02, 1.1897e-01],\n",
      "          [2.7076e-02, 9.9553e-02, 9.8192e-02,  ..., 7.5024e-02,\n",
      "           5.5968e-02, 2.6508e-01],\n",
      "          ...,\n",
      "          [3.6155e-02, 1.2723e-01, 6.7533e-02,  ..., 1.0634e-01,\n",
      "           8.7189e-02, 2.9372e-01],\n",
      "          [5.9843e-03, 4.8357e-02, 5.6483e-02,  ..., 3.6356e-02,\n",
      "           3.7911e-02, 6.7411e-01],\n",
      "          [1.0445e-02, 1.3440e-02, 1.4178e-02,  ..., 1.1608e-02,\n",
      "           1.3611e-02, 9.0476e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[6.8178e-02, 4.1510e-02, 1.5340e-01,  ..., 9.0423e-02,\n",
      "           3.6398e-02, 6.6535e-03],\n",
      "          [7.3738e-02, 1.9393e-01, 5.2897e-02,  ..., 1.8750e-01,\n",
      "           3.7326e-02, 1.2709e-01],\n",
      "          [3.0909e-02, 4.1361e-02, 2.3140e-01,  ..., 5.3291e-02,\n",
      "           5.3037e-02, 4.2319e-01],\n",
      "          ...,\n",
      "          [8.4556e-02, 6.3978e-02, 3.2097e-02,  ..., 1.2391e-01,\n",
      "           4.5760e-02, 3.7193e-01],\n",
      "          [4.8910e-02, 4.3844e-02, 1.5280e-01,  ..., 5.5409e-02,\n",
      "           1.0645e-01, 4.5647e-01],\n",
      "          [1.3956e-02, 1.5691e-02, 9.2559e-03,  ..., 9.4519e-03,\n",
      "           1.1977e-02, 9.1692e-01]],\n",
      "\n",
      "         [[1.8691e-03, 1.8472e-02, 7.5539e-02,  ..., 2.5675e-02,\n",
      "           4.5580e-02, 7.2320e-01],\n",
      "          [2.0809e-03, 2.2230e-02, 2.5896e-02,  ..., 1.9310e-02,\n",
      "           1.2992e-02, 8.5446e-01],\n",
      "          [3.7574e-03, 1.6275e-02, 2.1742e-02,  ..., 1.4251e-02,\n",
      "           1.1972e-02, 8.6976e-01],\n",
      "          ...,\n",
      "          [5.4291e-03, 7.4606e-03, 1.1539e-02,  ..., 1.5817e-02,\n",
      "           2.1992e-02, 7.9228e-01],\n",
      "          [4.1136e-03, 7.8082e-03, 8.1932e-03,  ..., 1.0209e-02,\n",
      "           1.3153e-02, 8.7540e-01],\n",
      "          [3.4726e-03, 4.0759e-03, 4.0077e-03,  ..., 5.1653e-03,\n",
      "           3.4651e-03, 9.5740e-01]],\n",
      "\n",
      "         [[5.1923e-03, 6.2877e-02, 2.6327e-01,  ..., 3.8901e-02,\n",
      "           7.5535e-02, 4.9378e-01],\n",
      "          [8.6042e-04, 2.3117e-02, 3.3364e-02,  ..., 2.6240e-02,\n",
      "           4.4145e-02, 8.6540e-01],\n",
      "          [1.8298e-03, 3.7182e-02, 1.0941e-01,  ..., 3.4414e-02,\n",
      "           7.1946e-02, 7.2658e-01],\n",
      "          ...,\n",
      "          [5.8875e-03, 2.9732e-02, 4.5056e-02,  ..., 4.1755e-02,\n",
      "           5.7954e-02, 7.9495e-01],\n",
      "          [6.6102e-03, 3.0845e-02, 6.2344e-02,  ..., 2.7815e-02,\n",
      "           4.6574e-02, 7.8731e-01],\n",
      "          [1.4080e-03, 1.3208e-02, 1.2877e-02,  ..., 1.1506e-02,\n",
      "           1.1516e-02, 9.3918e-01]]]], grad_fn=<SoftmaxBackward0>))\n"
     ]
    }
   ],
   "source": [
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb8ae37-a883-442f-a156-e5a141fcf05b",
   "metadata": {},
   "source": [
    "Pretraining and MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f07eeb0b-17bb-4fd1-a5a3-5785e3b7f30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0ab77ae0-cefa-4265-a208-ff2528798eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "084c4d52-219f-43d2-aa44-44cc998d2b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Deep Learning is subset of Machine Learning.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f5446e1c-9c5a-4820-ab19-6036a267a490",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, add_special_tokens=True)\n",
    "outputs = model(**inputs, labels=inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "44acf4e8-cd3c-4ef6-83f4-6b254aae737c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fdefff65-9fa4-4e49-b087-3340123763bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.7838, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a2443e-2ff8-45e7-b9b5-f91afbedc565",
   "metadata": {},
   "source": [
    "Extracting Word Embeddings with Hugging Face Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8330727c-85da-42f2-b0ba-c28f3d87640b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1e5430da-7dc5-47e0-bbd7-a45cf1accc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3c9a9d09-1549-4873-a959-e18e1a529e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"R.L is a feedback-based Machine Learning.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1fd3b432-144c-4885-a0dc-e6ece3df2258",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, add_special_tokens=True)\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "82e2ef40-d0d7-4f70-8f23-07a99f86833f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b5694139-7557-486c-911f-c7f111af579a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.5229, -0.1897, -0.0321,  ..., -0.2837, -0.0803,  0.7297],\n",
      "         [-0.0878, -0.3480,  0.3289,  ...,  0.1971,  0.7202,  0.1397],\n",
      "         [-0.9098, -0.3596,  0.5022,  ...,  0.6466,  0.4512,  0.0088],\n",
      "         ...,\n",
      "         [-0.7197, -0.2314, -0.1856,  ..., -0.9756, -0.3295,  0.5237],\n",
      "         [ 0.5293,  0.1158, -0.4517,  ...,  0.2678, -0.7594, -0.1698],\n",
      "         [ 0.2906,  0.2216, -0.2134,  ...,  0.3586, -0.9532, -0.0962]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(word_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5590796-1cd4-4801-8db1-44d3f02a44f1",
   "metadata": {},
   "source": [
    "Fine-Tuning Intermediate Layers with Hugging Face Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4bca3ff4-dd05-4a45-b07b-58f7d83f826f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4aa9c738-3593-4d1f-bdd1-0d552f107043",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "364a0835-6161-42db-ab03-1ab115bc0bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Advanced fine-tuning with BERT.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "99a21305-5176-40bb-93f9-aa9a73842611",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "outputs = model(**inputs, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f4721421-bcac-4057-8dd8-454cbf77b2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_layer = outputs.hidden_states[6]  # 7th layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "621644a4-b96c-4d6c-8f51-5392cb58d3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.3527, -1.1815, -0.3027,  ..., -0.1794,  0.0872,  0.5573],\n",
      "         [-0.3768,  0.1754,  0.3395,  ..., -0.0359,  0.2162, -1.2251],\n",
      "         [ 1.8406, -0.6458,  0.5841,  ..., -0.7345,  0.7542, -0.1614],\n",
      "         ...,\n",
      "         [ 1.0414, -0.7009,  1.0362,  ...,  1.0581, -0.3068, -1.4171],\n",
      "         [-0.8934, -0.8139, -0.3154,  ..., -0.3933, -0.6383,  0.0522],\n",
      "         [ 0.0143, -0.0423, -0.0131,  ...,  0.0044, -0.0140, -0.0394]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(intermediate_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d8fabf-0cd3-40b2-800b-d29e40ab0ecd",
   "metadata": {},
   "source": [
    " Using RoBERTa with Hugging Face Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2607d15f-0cd7-4044-a224-af9cf5f09fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4b25f958-2c01-4f4a-a9e9-a7370e61520e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0331d441e4c4257bfc2421686fdb62e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub\\models--roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f347668cd65147a388266aac32d603df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34834e7008124ab7bd7ccf2207fa9bbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "951ad74b4b43499aa9fc1ac6e901a2e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b79242fd3c4d5b909a13424a9d0df0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "157062f45a054e4aa8d1611d30067a14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaModel.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "50e497f0-fe6e-47f4-8f46-b05b0f1fcb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"RoBERTa is an advanced variant of BERT.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e4086bba-33f5-42b8-b408-a9ad59a3d2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6ee5561f-2b25-422b-8d19-171e9ff7740f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ed6d063a-aafc-4f68-9bfd-c26a5052e7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0640,  0.1073, -0.0181,  ..., -0.0383, -0.0555, -0.0151],\n",
      "         [-0.0647,  0.0450, -0.0528,  ...,  0.0814, -0.1633, -0.0284],\n",
      "         [ 0.0398,  0.0657, -0.1046,  ..., -0.1364, -0.0650,  0.0560],\n",
      "         ...,\n",
      "         [ 0.0273,  0.0271, -0.0042,  ..., -0.1554, -0.0037,  0.1105],\n",
      "         [-0.0560,  0.1078, -0.0385,  ..., -0.0619, -0.0614, -0.0363],\n",
      "         [ 0.0064,  0.1383,  0.0011,  ...,  0.0994, -0.0593,  0.0169]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a00f81-a19e-40fd-8b69-30faaad143be",
   "metadata": {},
   "source": [
    "Text Summarization using BERT with Hugging Face Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "475ac5b1-78fe-4225-9102-8a64b9917f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8cac4f19-cdc6-40d5-8628-ed01d7b7e65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4f0e44b0-09e0-4117-bfed-3ab67df9f086",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_text = \"Long text for summarization...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "19a7dbbb-5697-4e8c-861a-d88e6c077489",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(original_text, return_tensors='pt', padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9399143e-aa54-418c-966d-09975e289466",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_logits = model(**inputs).logits\n",
    "summary = tokenizer.decode(torch.argmax(summary_logits, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4d4c7f05-6e57-4c6a-b227-ea8285028369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: [PAD]\n"
     ]
    }
   ],
   "source": [
    "print(\"Summary:\", summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed25e19-2fcc-4cf6-a1b4-55e6ad7163ef",
   "metadata": {},
   "source": [
    "Handling Long Texts with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1d99abcf-9dae-4324-ab90-77f9b8f4314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 512  # Max token limit for BERT\n",
    "text = \"Random Forests is a supervised Machine Learning algorithm. Used in classification and regression problems\"\n",
    "text_chunks = [text[i:i + max_seq_length] for i in range(0, len(text), max_seq_length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "75144094-0dca-4635-ad9c-fc6cbcf76f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in text_chunks:\n",
    "    inputs = tokenizer(chunk, return_tensors='pt', padding=True, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "\n",
    " # Process outputs for each chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da07ac34-7a44-46b0-8328-5ef940ad199f",
   "metadata": {},
   "source": [
    "Mixed-Precision Training with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8beb074d-a2ab-4141-b497-9ec9c5dfb551",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2df34150-c6f4-48ca-84cd-72514c612343",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_12632\\4292142752.py:1: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7b84dc5f-c4cd-4137-9966-12ec39845285",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_12632\\3333037650.py:1: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:265: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with autocast():\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    loss = outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a243bc31-baa9-4455-b9b3-d11e77f61287",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[124], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\n\u001b[1;32m----> 2\u001b[0m scaler\u001b[38;5;241m.\u001b[39mstep(optimizer)\n\u001b[0;32m      3\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "scaler.scale(loss).backward()\n",
    "scaler.step(optimizer)\n",
    "scaler.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26233f6-1149-44f5-97be-162dc60ee81f",
   "metadata": {},
   "source": [
    "Domain Adaptation with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0099e107-8c1e-4ee5-a5a2-e5ac86f71494",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_domain_specific_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[126], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m domain_data \u001b[38;5;241m=\u001b[39m load_domain_specific_data()  \u001b[38;5;66;03m# Load domain-specific dataset\u001b[39;00m\n\u001b[0;32m      2\u001b[0m domain_model \u001b[38;5;241m=\u001b[39m BertForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m train_domain(domain_model, domain_data)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_domain_specific_data' is not defined"
     ]
    }
   ],
   "source": [
    "domain_data = load_domain_specific_data()  # Load domain-specific dataset\n",
    "domain_model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "train_domain(domain_model, domain_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b126768-f0fc-4e42-ae5c-3f6043467543",
   "metadata": {},
   "source": [
    "Multilingual BERT with Hugging Face Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f30352e8-145f-4731-9641-2f2cae271e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "793738f562d6416b954584be1f20731f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub\\models--bert-base-multilingual-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d96ddac4115a411896a86fecb3f189e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0eae7f9c39749b3a637f450e6898c14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3368e87a6d4340338c3dee0870a4c66d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8654fe7001cc4cfaabaf26883e0fceee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1740, -0.3340, -0.6609,  ...,  0.4222,  0.3596, -0.0141],\n",
      "         [ 0.4543,  0.1575, -0.2935,  ..., -0.0887,  0.3577,  0.3759],\n",
      "         [ 0.7044,  0.1681, -0.1217,  ...,  0.9337,  0.4014, -0.2957],\n",
      "         ...,\n",
      "         [-0.2753, -0.4430, -0.5777,  ...,  0.4702,  0.0326, -0.1780],\n",
      "         [-0.4562, -0.2416, -0.9867,  ...,  0.1909,  0.5182, -0.1369],\n",
      "         [-0.4752, -0.3440, -0.5101,  ...,  0.0674,  0.5783, -0.1292]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "text = \"BERT understands multiple languages!\"\n",
    "inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "outputs = model(**inputs)\n",
    "\n",
    "embeddings = outputs.last_hidden_state\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3b2c5e-f881-43b8-9b34-a1d39eacea60",
   "metadata": {},
   "source": [
    "Lifelong Learning with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b72ee8fa-9b6d-400a-8442-498fbcfb58d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'load_latest_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[130], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m BertForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m new_data \u001b[38;5;241m=\u001b[39m load_latest_data()  \u001b[38;5;66;03m# Load updated dataset\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     10\u001b[0m     train_lifelong(model, new_data)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_latest_data' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "new_data = load_latest_data()  # Load updated dataset\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_lifelong(model, new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134564b9-740f-4101-94dd-1b5d71487a27",
   "metadata": {},
   "source": [
    "Implementing BERT with Hugging Face Transformers Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b5e02dcb-60e7-4879-871b-c60b3c935c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "8dca1239-876c-4700-b063-386bdd493bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"BERT is amazing!\"\n",
    "inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b719d6b1-8cf4-40fb-834c-1b598c245d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sentiment Class: 1\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "predicted_class = torch.argmax(outputs.logits).item()\n",
    "print(\"Predicted Sentiment Class:\", predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9b497bcc-bf17-40c1-8c1b-d828c7c4b3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer, AdamW\n",
    "import torch\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "text = \"Sample text for training.\"\n",
    "label = 1  # Assuming positive sentiment\n",
    "\n",
    "inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "outputs = model(**inputs, labels=torch.tensor([label]))\n",
    "\n",
    "loss = outputs.loss\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a45f3d9-fb99-49e0-aafc-b17bbffc6dde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03862dc-04f4-4d49-84f7-16f10049be81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b832a32a-0fe2-42f4-9a6b-007287a98308",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
